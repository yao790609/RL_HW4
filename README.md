# ğŸ“ 4-1 DQN + Experience Replay - Understanding Report

## âœ… Assignment Objectives

### Task Checklist
- âœ… Run the provided code (naive or with replay buffer)
- âœ… Discuss with ChatGPT to clarify code understanding
- âœ… Submit a short report including:
  - Basic DQN implementation for a simple environment
  - Use of Experience Replay Buffer

---

## 1ï¸âƒ£ Code Execution and Understanding âœ…

### ğŸ§© Environment and Model
- The environment is a custom `Gridworld` (4x4).
- Mode used: `'static'` (all elements are fixed).
- State input is a flattened 4x4 grid (1x64) with slight noise added for generalization.

### ğŸ§  Model Architecture
- A fully connected neural network:
  - Input: 64-dim (flattened state)
  - Hidden layers: 150 â†’ 100 â†’ 4 (Q-values for 4 actions)
  - Activation: ReLU

---

## 2ï¸âƒ£ Key DQN Techniques ğŸ’¡

### ğŸ”¹ Îµ-greedy Strategy (Exploration vs Exploitation)
- Random action with Îµ = 0.3 probability (exploration).
- Otherwise, choose the action with highest Q-value (exploitation).
- Balances discovering new actions and optimizing known good ones.

### ğŸ”¹ Experience Replay
- Implemented via `collections.deque` to store the most recent 1000 experiences:  
  `(state, action, reward, next_state, done)`
- During training, randomly sample 200 experiences (mini-batch) to break time correlation and improve diversity.

### ğŸ”¹ Target Network
- `model2` is a deep copy of the main model.
- Every `sync_freq = 500` steps, synchronize weights from the main model to the target model to stabilize learning.

### ğŸ”¹ Loss Function & Optimizer
- Loss is calculated using `MSELoss`:
L = MSE(Q(s, a), r + Î³ * max_a' Q_target(s'))

- Optimizer: Adam with learning rate = 1e-3

---

## 3ï¸âƒ£ Training Loop Overview ğŸ”

### ğŸ”„ Naive or Replay DQN Training Steps:
1. Initialize the Gridworld environment.
2. Obtain the current state `state1` with added noise.
3. Use the model to predict Q-values and select an action.
4. Execute the action, receive `reward` and `state2`.
5. Save the experience to the replay memory.
6. If experience buffer â‰¥ 200:
 - Randomly sample a batch
 - Compute target Q-values
 - Calculate loss and backpropagate
7. Every 500 steps, sync model weights to target network.
8. End episode when reward â‰  -1 or steps > 50, then reset environment.

---

## 4ï¸âƒ£ Key Concepts Discussed with ChatGPT ğŸ“Œ

| Topic | Key Insights |
|-------|--------------|
| `render_np()` output | Although rendered every time, `state2` is rendered **after** `makeMove()`, ensuring it's the result of the action taken |
| Noise | Added via `+ np.random.rand()/10.0` for robustness, without significantly altering the state |
| `state_` vs `state2` | `state_` is the current state before action, `state2` is the result of the action (temporal continuity) |
| Why add noise | Small noise helps avoid overfitting to exact pixel values |
| Purpose of `model2` | Acts as a target network to stabilize learning by decoupling target calculation |
| Replay Buffer purpose | Prevents overfitting to recent samples, improves convergence through better sample diversity |

---

## âœ… Summary

This assignment demonstrates how to implement a DQN agent on a simple Gridworld environment, transitioning from naive to experience replay-enhanced training.

You should now understand:

- How to construct and train a basic DQN network
- The need for experience replay and a target network
- How Îµ-greedy helps with exploration
- How to stabilize and improve training with proper sample handling and network updates

---


# 4-2 å¼·åŒ–å­¸ç¿’æ¨¡å‹æ¯”è¼ƒï¼šDQN / Double DQN / Dueling Double DQN

æœ¬å°ˆæ¡ˆæ¯”è¼ƒä¸‰ç¨®å¼·åŒ–å­¸ç¿’æ¨¡å‹åœ¨ 4x4 GridWorld ä»»å‹™ä¸Šçš„è¡¨ç¾ï¼š

- **DQN**ï¼šåŸºç¤æ·±åº¦ Q ç¶²è·¯ã€‚
- **Double DQN**ï¼šåŠ å…¥å…©å€‹ Q ç¶²è·¯é˜²æ­¢ Q-value éåº¦ä¼°è¨ˆã€‚
- **Dueling Double DQN**ï¼šé€²ä¸€æ­¥åˆ†é›¢ã€Œç‹€æ…‹åƒ¹å€¼ã€èˆ‡ã€Œå„ªå‹¢å‡½æ•¸ã€ä»¥æå‡ç©©å®šæ€§ã€‚

---

## ğŸ§  æ¨¡å‹æ¶æ§‹æ¯”è¼ƒ

| æ¨¡å‹é¡å‹        | æ ¸å¿ƒçµæ§‹                                             | å„ªé»èªªæ˜                                      |
|-----------------|------------------------------------------------------|-----------------------------------------------|
| DQN             | å–®ä¸€ Q ç¶²è·¯ï¼Œç›´æ¥è¼¸å‡ºæ‰€æœ‰å‹•ä½œçš„ Q å€¼                | æ¶æ§‹ç°¡å–®ï¼Œé©åˆæ–°æ‰‹å…¥é–€                        |
| Double DQN      | ä½¿ç”¨ Target Q ç¶²è·¯ä¾†è§£è€¦é¸æ“‡èˆ‡è©•ä¼°                   | å¯é¿å… Q-value éä¼°                           |
| Dueling Double DQN | æ‹†åˆ†ç‚ºã€Œç‹€æ…‹åƒ¹å€¼ã€èˆ‡ã€Œå„ªå‹¢å‡½æ•¸ã€å…©å€‹åˆ†æ”¯             | æå‡å­¸ç¿’ç©©å®šæ€§èˆ‡æ•ˆèƒ½                          |

> ğŸ“Œ ç¯„ä¾‹æ¨¡å‹æ¶æ§‹åœ–å»ºè­°æ”¾åœ¨ `docs/images/architecture.png`ï¼Œå¯ç”¨ `![æ¨¡å‹æ¶æ§‹](docs/images/architecture.png)` æ’å…¥ã€‚

---

## ğŸ“Š æ¨¡å‹è¨“ç·´æ•ˆèƒ½æ¯”è¼ƒï¼ˆç¸½å›åˆæ•¸ï¼š1000ï¼‰

| æ¨¡å‹             | å¹³å‡ Lossï¼ˆæœ€å¾Œ 100 å›åˆï¼‰ | æ”¶æ–‚é€Ÿåº¦ï¼ˆç´„ç¬¬ N å›åˆï¼‰ | å‚™è¨»                       |
|------------------|-----------------------------|---------------------------|----------------------------|
| **DQN**          | ~0.025                      | ç´„ 650 å›åˆ               | æ³¢å‹•å¤§ï¼Œå¸¸é™·å…¥å±€éƒ¨æœ€å„ª     |
| **Double DQN**   | ~0.015                      | ç´„ 500 å›åˆ               | æ›´ç©©å®šï¼Œæ”¶æ–‚é€Ÿåº¦è¼ƒå¿«       |
| **Dueling DQN**  | ~0.010                      | ç´„ 400 å›åˆ               | æœ€ç©©å®šï¼Œä¸”è¨“ç·´æ›²ç·šå¹³æ»‘     |

---

## ğŸ“ˆ è¨“ç·´æå¤±æ›²ç·šåœ–

ä¸‰ç¨®æ¨¡å‹çš„æå¤±å‡½æ•¸è¶¨å‹¢åœ–å¦‚ä¸‹ï¼š

![Training Loss Comparison](docs/images/loss_comparison.png)

> `loss_comparison.png` å»ºè­°æ”¾åœ¨ `/docs/images/` è·¯å¾‘ä¸‹ã€‚

---

## ğŸ“ å°ˆæ¡ˆçµæ§‹èªªæ˜
â”œâ”€â”€ dqn_original.py # åŸå§‹ DQN å¯¦ä½œ
â”œâ”€â”€ double_dqn.py # Double DQN å¯¦ä½œ
â”œâ”€â”€ dueling_dqn.py # Dueling Double DQN å¯¦ä½œ
â”œâ”€â”€ compare_plot.py # ç¹ªåœ–ï¼šä¸‰æ¨¡å‹æ¯”è¼ƒ
â”œâ”€â”€ docs/
â”‚ â””â”€â”€ images/
â”‚ â”œâ”€â”€ architecture.png
â”‚ â””â”€â”€ loss_comparison.png
â””â”€â”€ README.md

---

## ğŸ“Œ ç’°å¢ƒéœ€æ±‚

- Python 3.8+
- PyTorch 2.0+
- matplotlib


